{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "scratchpad",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vasudev-sharma/Flixstock-assignment/blob/master/Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BKG1B7dOExG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9340bff0-7e51-438f-c143-97bb9cfad082"
      },
      "source": [
        "# Download the data\n",
        "!FILEID='1p7-dU6rDuqZ2mxv5ac5AWndt4z19aS6j' && \\\n",
        "FILENAME='data.zip' && \\\n",
        "FILEDEST=\"https://docs.google.com/uc?export=download&id=${FILEID}\" && \\\n",
        "wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate ${FILEDEST} -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=${FILEID}\" -O $FILENAME && rm -rf /tmp/cookies.txt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-06-14 12:41:18--  https://docs.google.com/uc?export=download&id=1p7-dU6rDuqZ2mxv5ac5AWndt4z19aS6j\n",
            "Resolving docs.google.com (docs.google.com)... 74.125.23.102, 74.125.23.100, 74.125.23.113, ...\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.23.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘STDOUT’\n",
            "\n",
            "-                       [ <=>                ]   3.22K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-06-14 12:41:19 (31.4 MB/s) - written to stdout [3300]\n",
            "\n",
            "--2021-06-14 12:41:19--  https://docs.google.com/uc?export=download&confirm=gQ4o&id=1p7-dU6rDuqZ2mxv5ac5AWndt4z19aS6j\n",
            "Resolving docs.google.com (docs.google.com)... 74.125.204.101, 74.125.204.139, 74.125.204.138, ...\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.204.101|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0s-44-docs.googleusercontent.com/docs/securesc/htnjk5a03n8r0aaeu9ngrpnmo3d471nf/9t69rokbrojdni0qmhrsr5m8uihoc01l/1623674475000/03365649102232098283/17256998712766813162Z/1p7-dU6rDuqZ2mxv5ac5AWndt4z19aS6j?e=download [following]\n",
            "--2021-06-14 12:41:19--  https://doc-0s-44-docs.googleusercontent.com/docs/securesc/htnjk5a03n8r0aaeu9ngrpnmo3d471nf/9t69rokbrojdni0qmhrsr5m8uihoc01l/1623674475000/03365649102232098283/17256998712766813162Z/1p7-dU6rDuqZ2mxv5ac5AWndt4z19aS6j?e=download\n",
            "Resolving doc-0s-44-docs.googleusercontent.com (doc-0s-44-docs.googleusercontent.com)... 74.125.23.132, 2404:6800:4008:c02::84\n",
            "Connecting to doc-0s-44-docs.googleusercontent.com (doc-0s-44-docs.googleusercontent.com)|74.125.23.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://docs.google.com/nonceSigner?nonce=vp0392todqof4&continue=https://doc-0s-44-docs.googleusercontent.com/docs/securesc/htnjk5a03n8r0aaeu9ngrpnmo3d471nf/9t69rokbrojdni0qmhrsr5m8uihoc01l/1623674475000/03365649102232098283/17256998712766813162Z/1p7-dU6rDuqZ2mxv5ac5AWndt4z19aS6j?e%3Ddownload&hash=euvp9kdr543hbsnih88urbed13ij9h6t [following]\n",
            "--2021-06-14 12:41:19--  https://docs.google.com/nonceSigner?nonce=vp0392todqof4&continue=https://doc-0s-44-docs.googleusercontent.com/docs/securesc/htnjk5a03n8r0aaeu9ngrpnmo3d471nf/9t69rokbrojdni0qmhrsr5m8uihoc01l/1623674475000/03365649102232098283/17256998712766813162Z/1p7-dU6rDuqZ2mxv5ac5AWndt4z19aS6j?e%3Ddownload&hash=euvp9kdr543hbsnih88urbed13ij9h6t\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.204.101|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://doc-0s-44-docs.googleusercontent.com/docs/securesc/htnjk5a03n8r0aaeu9ngrpnmo3d471nf/9t69rokbrojdni0qmhrsr5m8uihoc01l/1623674475000/03365649102232098283/17256998712766813162Z/1p7-dU6rDuqZ2mxv5ac5AWndt4z19aS6j?e=download&nonce=vp0392todqof4&user=17256998712766813162Z&hash=i0leioipfaj8mamtqsr7e7on73643ftj [following]\n",
            "--2021-06-14 12:41:19--  https://doc-0s-44-docs.googleusercontent.com/docs/securesc/htnjk5a03n8r0aaeu9ngrpnmo3d471nf/9t69rokbrojdni0qmhrsr5m8uihoc01l/1623674475000/03365649102232098283/17256998712766813162Z/1p7-dU6rDuqZ2mxv5ac5AWndt4z19aS6j?e=download&nonce=vp0392todqof4&user=17256998712766813162Z&hash=i0leioipfaj8mamtqsr7e7on73643ftj\n",
            "Connecting to doc-0s-44-docs.googleusercontent.com (doc-0s-44-docs.googleusercontent.com)|74.125.23.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/zip]\n",
            "Saving to: ‘data.zip’\n",
            "\n",
            "data.zip                [  <=>               ]  71.57M   189MB/s    in 0.4s    \n",
            "\n",
            "2021-06-14 12:41:20 (189 MB/s) - ‘data.zip’ saved [75043420]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLC_XF_DRMS3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90acc939-5713-49d3-8336-d58d442e483b"
      },
      "source": [
        "# unzip the data\n",
        "%%bash\n",
        "unzip data.zip -d data\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  data.zip\n",
            "  inflating: data/classification-assignment/images/Thumbs.db  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "replace data/__MACOSX/._classification-assignment? [y]es, [n]o, [A]ll, [N]one, [r]ename:  NULL\n",
            "(EOF or read error, treating as \"[N]one\" ...)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMfcQpkBRsaz"
      },
      "source": [
        "\n",
        "# PyTorch imports\n",
        "import torch\n",
        "from torchvision import transforms, datasets\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import math\n",
        "import random"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZzZ-9Yi5WRb",
        "outputId": "4f70f7f7-fe62-4b12-e5c5-f3eba7e13db5"
      },
      "source": [
        "!pip install wandb\n",
        "import wandb\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.7/dist-packages (0.10.32)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.12.4)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: sentry-sdk>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.1.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: configparser>=3.8.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.0.2)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: subprocess32>=3.5.3 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.5.4)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.1)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.1.17)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.12.0->wandb) (57.0.0)\n",
            "Requirement already satisfied: urllib3>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from sentry-sdk>=0.4.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from sentry-sdk>=0.4.0->wandb) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.0.7)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.0; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (3.7.4.3)\n",
            "Requirement already satisfied: smmap<5,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (4.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzjN66uslLd6"
      },
      "source": [
        "data_path = Path('/content/data/classification-assignment')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Twdiwq-b-dlv",
        "outputId": "5fc15d84-38cd-42fc-cc52-e42869b34352"
      },
      "source": [
        "# Remove `Thumbs.db` file from images directory\n",
        "path_file = data_path / 'images' / 'Thumbs.db'\n",
        "if os.path.exists(str(path_file)):\n",
        "  print(\"The file exists\")\n",
        "  os.unlink(str(path_file))\n",
        "  print('Thumbs.db file has been removed')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The file exists\n",
            "Thumbs.db file has been removed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9w8dMPmlsSA"
      },
      "source": [
        "# transforms \n",
        "train_transforms =  transforms.Compose(\n",
        "    [\n",
        "     transforms.Resize((256, 256)),\n",
        "     transforms.RandomHorizontalFlip(),\n",
        "     transforms.ToTensor(),\n",
        "     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ]\n",
        "      )\n",
        "\n",
        "valid_transforms = transforms.Compose(\n",
        "          [transforms.Resize((256, 256)),\n",
        "           transforms.RandomHorizontalFlip(),\n",
        "           transforms.ToTensor(),\n",
        "           transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "           ]\n",
        "                                      )\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nh14MDFBkH8G"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZxS7_TUTItH"
      },
      "source": [
        "class FashionDataset(Dataset):\n",
        "  def __init__(self, root, transform=None, target_transform=None):\n",
        "    self.root = root\n",
        "    self.transform = transform\n",
        "    self.target_transform = target_transform\n",
        "    self.samples = os.listdir(str(self.root / 'images'))\n",
        "\n",
        "    # Attributes dataframe\n",
        "    self.df_attributes = pd.read_csv(str(self.root / 'attributes.csv'))\n",
        "\n",
        "    # For each attribute, one hot encode it\n",
        "    self.preprocess_targets() \n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.samples)\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "\n",
        "    # shuffle the images list\n",
        "    np.random.shuffle(self.samples)\n",
        "\n",
        "    # name of the image\n",
        "    filename_image = self.samples[index]\n",
        "\n",
        "    try:\n",
        "      image = Image.open(str(self.root / 'images' / filename_image)).convert('RGB')\n",
        "    except Exception as e:\n",
        "      print('Path of the image is', str(self.root / 'images' / filename_image))\n",
        "      print('Unable to read the image')\n",
        "\n",
        "    # retreive the specific row of given index\n",
        "    df_row = self.df_attributes.loc[self.df_attributes['filename'] == filename_image]\n",
        "\n",
        "    if self.transform is not None:\n",
        "      image = self.transform(image)\n",
        "\n",
        "    # Target\n",
        "    target_start_idx = self.df_attributes.columns.get_loc('neck_0.0')\n",
        "    target = torch.tensor(self.df_attributes.iloc[0].tolist()[target_start_idx:], dtype=torch.float32)\n",
        "\n",
        "    # return target and labels\n",
        "    return image, target\n",
        "  \n",
        "  def preprocess_targets(self):\n",
        "    # Drop rows which have na\n",
        "    self.df_attributes = self.df_attributes.dropna()\n",
        "\n",
        "    # one hot encode the Neck attribute\n",
        "    one_hot_neck = pd.get_dummies(self.df_attributes.neck, prefix='neck')\n",
        "\n",
        "    # one hot encode the sleeve_length attribute\n",
        "    one_hot_sleeve_length = pd.get_dummies(self.df_attributes.sleeve_length, prefix='sleeve_length')\n",
        "\n",
        "    # one hot encode the patter attribute\n",
        "    one_hot_pattern = pd.get_dummies(self.df_attributes.pattern, prefix='pattern')\n",
        "\n",
        "    # concatenate the one hot encoded attributes to dataframe\n",
        "    self.df_attributes = pd.concat([self.df_attributes, one_hot_neck, one_hot_sleeve_length, one_hot_pattern], axis=1)\n",
        "\n",
        "\n",
        "  \n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psa_EyjUWigW"
      },
      "source": [
        "dataset = FashionDataset(data_path, transform=train_transforms)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DW7r24Q35LWl"
      },
      "source": [
        "# split of dataset into train, valid, test datasets\n",
        "\n",
        "train_split = 0.75\n",
        "val_split = 0.15\n",
        "test_split = 1 - (train_split + val_split)\n",
        "\n",
        "len_train_dataset = math.ceil(train_split * len(dataset))\n",
        "len_val_dataset = math.ceil(val_split * len(dataset))\n",
        "len_test_dataset = len(dataset) - (len_train_dataset + len_val_dataset)\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOl8S6yB57KK"
      },
      "source": [
        "ds_train, ds_valid, ds_test = random_split(dataset, [len_train_dataset, len_val_dataset, len_test_dataset])"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeZ3B-1v6SiV"
      },
      "source": [
        "# sanity check \n",
        "assert len(ds_train) == len_train_dataset\n",
        "assert len(ds_valid) == len_val_dataset\n",
        "assert len(ds_test) == len_test_dataset"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbrGmd_T6zhD"
      },
      "source": [
        "# hyperparameters\n",
        "batch_size = 16\n",
        "lr = 5e-5\n",
        "epochs=30\n",
        "log_freq = 10\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "n_labels = 21\n",
        "model_name = 'resnet50'\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWBHG7smRuME"
      },
      "source": [
        "model_dict = {\n",
        "    # DenseNet\n",
        "    'densenet121': 1024,\n",
        "    'densenet169': 1664,\n",
        "    'densenet161': 2208,\n",
        "\n",
        "    # ResNet\n",
        "    'resnet50': 2048,\n",
        "    'resnet101' : 2048,\n",
        "    'resnet34': 512,\n",
        "\n",
        "    # EfficientNet\n",
        "    'efficientnet-b0':1280,\n",
        "    'efficientnet-b3': 1536,\n",
        "    'efficientnet-b5': 2048\n",
        "\n",
        "}"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21Esn19IGTHH",
        "outputId": "bcfc7d9b-30a8-4dc7-cc0a-03da9aef66ad"
      },
      "source": [
        "device"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1swNMGXJ6fWm"
      },
      "source": [
        "# Dataloaders\n",
        "dl_train = DataLoader(ds_train, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
        "dl_valid = DataLoader(ds_valid, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
        "dl_test = DataLoader(ds_valid, batch_size=len(ds_test), shuffle=False, pin_memory=True)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxWYTOgR7elY",
        "outputId": "ca4e0673-805e-4736-f948-a3e0e93bf820"
      },
      "source": [
        "# sanity check the shape \n",
        "for batch in dl_train:\n",
        "  img_batch, target_batch = batch\n",
        "  print(img_batch.shape)\n",
        "  print(target_batch.shape)\n",
        "  break"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 21])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "maoB_Flr8fz9",
        "outputId": "759aebf9-7598-4d63-bc43-6fdab224d760"
      },
      "source": [
        "#@title\n",
        "!wandb login 202040aaac395bbf5a4a47d433a5335b74b7fb0e"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2KlsF8HYQBA"
      },
      "source": [
        ""
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmVitCVOCBxf"
      },
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZ1J-LdhRsiS",
        "outputId": "7a268285-b3af-48e8-9997-aab1bc4c66bd"
      },
      "source": [
        "# efficient net model -> https://github.com/lukemelas/EfficientNet-PyTorch\n",
        "!pip install efficientnet_pytorch"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: efficientnet_pytorch in /usr/local/lib/python3.7/dist-packages (0.7.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from efficientnet_pytorch) (1.8.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch->efficientnet_pytorch) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->efficientnet_pytorch) (3.7.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcZS6p5UCGkF",
        "outputId": "8c6fd483-39df-4d19-d794-76666ec1bfb4"
      },
      "source": [
        "# load a pretrained Densenet 121 model for finetuing on the Chest X_ray images\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "\n",
        "if not model_name.startswith('efficientnet'):\n",
        "  model = torch.hub.load('pytorch/vision:v0.9.0', model_name, pretrained=True)\n",
        "else:\n",
        "  model = EfficientNet.from_pretrained(model_name)\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.9.0\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ip-xSekZDcak",
        "outputId": "efff8d11-8699-45a4-e24d-36a8cbb66f3a"
      },
      "source": [
        "# Densenet \n",
        "if model_name.startswith('densenet'): \n",
        "  model.classifier = nn.Linear(model_dict[model_name], n_labels)\n",
        "\n",
        "if model_name.startswith('efficientnet'):\n",
        "  print(model_dict[model_name])\n",
        "  model._fc = nn.Linear(model_dict[model_name], n_labels)\n",
        "else:\n",
        "  model.fc = nn.Linear(model_dict[model_name], n_labels)\n",
        "\n",
        "# Migrate the mode to device\n",
        "model.to(device)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (4): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (5): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=2048, out_features=21, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOs3mu4_HhkQ"
      },
      "source": [
        "model.loss_func = nn.BCELoss()\n",
        "model.optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "model.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(model.optimizer, T_max=5)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utoi8RX6NFmo"
      },
      "source": [
        "from sklearn.metrics import *"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eg3B7P5_MIe3"
      },
      "source": [
        "def calculate_metrics(pred, target, threshold=0.5):\n",
        "  pred = np.array(pred > threshold, dtype = float)\n",
        "  return { \n",
        "      # Micro scores\n",
        "      'micro/precision':precision_score(y_true=target, y_pred=pred, average='micro'),\n",
        "      'micro/recall': recall_score(y_true=target, y_pred=pred, average='micro'),\n",
        "      'micro/f1_score': f1_score(y_true=target, y_pred=pred, average='micro'),\n",
        "\n",
        "      # Macro scores\n",
        "      'macro/precision':precision_score(y_true=target, y_pred=pred, average='macro'),\n",
        "      'macro/recall': recall_score(y_true=target, y_pred=pred, average='macro'),\n",
        "      'macro/f1_score': f1_score(y_true=target, y_pred=pred, average='macro'),\n",
        "\n",
        "  }"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXLmF6lqNLsC"
      },
      "source": [
        "def train_model(model, dl_train, dl_valid, epochs, log_freq):\n",
        "  print('***************Training of the model has started**************')\n",
        "  wandb.watch(model, log='all')\n",
        "\n",
        "  iteration = 0\n",
        "  for epoch in range(epochs+1):\n",
        "\n",
        "    batch_losses = []\n",
        "    preds_list = []\n",
        "    targets_list = []\n",
        "    for batch_idx, (features, targets) in enumerate(dl_train, 1):\n",
        "      model.train()\n",
        "      \n",
        "      model.optimizer.zero_grad()\n",
        "\n",
        "      # Migrate the features and targets to device\n",
        "      features, targets = features.to(device), targets.to(device)\n",
        "\n",
        "      # Forward pass\n",
        "      logits = model(features)\n",
        "      preds = torch.sigmoid(logits)\n",
        "\n",
        "      # loss\n",
        "      loss = model.loss_func(preds, targets)\n",
        "\n",
        "      # metrics\n",
        "      preds_list.extend(preds.cpu().detach().numpy())\n",
        "      targets_list.extend(targets.cpu().detach().numpy())\n",
        "\n",
        "      # backward pass\n",
        "      loss.backward()\n",
        "\n",
        "      # update weights\n",
        "      model.optimizer.step()\n",
        "\n",
        "      batch_losses.append(loss.item())\n",
        "    \n",
        "    loss_mean = np.mean(batch_losses)\n",
        "    metrics_dict = calculate_metrics(np.array(preds_list), np.array(targets_list))\n",
        "    \n",
        "    print('[Epoch = % d]: train_loss = %.3f,  micro/precision = %.3f, micro/recall = %.3f, micro/f1_score = %.3f, macro/precision = %.3f, macro/recall = %.3f, macro/f1_score = %.3f' \\\n",
        "          % (epoch, loss_mean , metrics_dict['micro/precision'], metrics_dict['micro/recall'], metrics_dict['micro/f1_score'], metrics_dict['macro/precision'], metrics_dict['macro/recall'], metrics_dict['macro/f1_score']))\n",
        "\n",
        "    print('====='*12)\n",
        "    print('\\n')\n",
        "\n",
        "    # Validation \n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "\n",
        "      val_preds_list = []\n",
        "      val_targets_list = []\n",
        "      val_loss_sum = 0.0\n",
        "      for val_batch_idx, (val_features, val_targets) in enumerate(dl_valid, 1):\n",
        "          \n",
        "          val_features, val_targets = val_features.to(device), val_targets.to(device)\n",
        "          \n",
        "          # Forward pass\n",
        "          val_logits = model(val_features)\n",
        "          val_preds = torch.sigmoid(val_logits)\n",
        "\n",
        "          val_preds_list.extend(val_preds.cpu().numpy())\n",
        "          val_targets_list.extend(val_targets.cpu().numpy())\n",
        "\n",
        "          # val loss\n",
        "          val_loss = model.loss_func(val_preds, val_targets).item()\n",
        "          val_loss_sum+=val_loss\n",
        "\n",
        "    val_metrics_dict = calculate_metrics(np.array(val_preds_list), np.array(val_targets_list))\n",
        "\n",
        "\n",
        "    print('-> [Epoch = % d]: val_loss = %.3f,  val_micro/precision = %.3f, val_micro/recall = %.3f, val_micro/f1_score = %.3f, val_macro/precision = %.3f, val_macro/recall = %.3f, val_macro/f1_score = %.3f' \\\n",
        "          % (epoch, val_loss_sum / val_batch_idx, val_metrics_dict['micro/precision'], val_metrics_dict['micro/recall'], val_metrics_dict['micro/f1_score'], val_metrics_dict['macro/precision'], val_metrics_dict['macro/recall'], val_metrics_dict['macro/f1_score']))\n",
        "    print('----'*12)\n",
        "    print('\\n')\n",
        "\n",
        "    wandb.log({'epoch': epoch,\n",
        "               'train_loss': loss_mean,\n",
        "               'train_micro_precision': metrics_dict['micro/precision'],\n",
        "                'train_micro_recall':metrics_dict['micro/recall'],\n",
        "                'train_micro_f1_score': metrics_dict['micro/f1_score'],\n",
        "                'train_macro_precision':metrics_dict['macro/precision'],\n",
        "                'train_macro_recall': metrics_dict['macro/recall'],\n",
        "                 'train_macro_f1_score': metrics_dict['macro/f1_score'],\n",
        "                 'val_loss' : val_loss_sum / val_batch_idx,\n",
        "                'val_micro_precision': val_metrics_dict['micro/precision'],\n",
        "                'val_micro_recall': val_metrics_dict['micro/recall'],\n",
        "                'val_micro_f1_score': val_metrics_dict['micro/f1_score'],\n",
        "                'val_macro_precision': val_metrics_dict['macro/precision'],\n",
        "                'val_macro_recall': val_metrics_dict['macro/recall'],\n",
        "                'val_macro_f1_score': val_metrics_dict['macro/f1_score']\n",
        "\n",
        "              })\n",
        "\n",
        "  \n",
        "  print('****************Model training completed******************')\n",
        "  torch.save(model.state_dict(), 'model.h5')\n",
        "\n",
        "\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fbgA2inKCcRT",
        "outputId": "b28e820b-26c7-4322-998c-308bdfd8dc66"
      },
      "source": [
        "def main():\n",
        "  # wandb initialize a new run\n",
        "  wandb.init(project='Flixstock-assignment')\n",
        "  wandb.watch_called = False\n",
        "\n",
        "  config = wandb.config\n",
        "  config.batch_size = batch_size\n",
        "  config.epochs = epochs\n",
        "  config.lr = lr\n",
        "  config.seed = 42\n",
        "  config.labels = n_labels\n",
        "  config.device = device\n",
        "  config.model_name = model_name\n",
        "  config.log_freq = log_freq\n",
        "\n",
        "  # set seed and set cuddn to deterministic for reproducible results\n",
        "  torch.manual_seed(config.seed)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "\n",
        "  np.random.seed(config.seed)\n",
        "  random.seed(config.seed)\n",
        "\n",
        "  train_model(model, dl_train, dl_valid, epochs, log_freq)\n",
        "\n",
        " \n",
        "if __name__=='__main__':\n",
        "  main()\n",
        "  wandb.finish()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvs74\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.32<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">curious-grass-42</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/vs74/Flixstock-assignment\" target=\"_blank\">https://wandb.ai/vs74/Flixstock-assignment</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/vs74/Flixstock-assignment/runs/1ui3m268\" target=\"_blank\">https://wandb.ai/vs74/Flixstock-assignment/runs/1ui3m268</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210614_124135-1ui3m268</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "***************Training of the model has started**************\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1515: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  average, \"true nor predicted\", 'F-score is', len(true_sum)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch =  0]: train_loss = 0.181,  micro/precision = 0.874, micro/recall = 0.983, micro/f1_score = 0.925, macro/precision = 0.143, macro/recall = 0.140, macro/f1_score = 0.142\n",
            "============================================================\n",
            "\n",
            "\n",
            "[Epoch =  0]: val_loss = 0.033,  val_micro/precision = 1.000, val_micro/recall = 1.000, val_micro/f1_score = 1.000, val_macro/precision = 0.143, val_macro/recall = 0.143, val_macro/f1_score = 0.143\n",
            "------------------------------------------------\n",
            "\n",
            "\n",
            "[Epoch =  1]: train_loss = 0.018,  micro/precision = 1.000, micro/recall = 1.000, micro/f1_score = 1.000, macro/precision = 0.143, macro/recall = 0.143, macro/f1_score = 0.143\n",
            "============================================================\n",
            "\n",
            "\n",
            "[Epoch =  1]: val_loss = 0.010,  val_micro/precision = 1.000, val_micro/recall = 1.000, val_micro/f1_score = 1.000, val_macro/precision = 0.143, val_macro/recall = 0.143, val_macro/f1_score = 0.143\n",
            "------------------------------------------------\n",
            "\n",
            "\n",
            "[Epoch =  2]: train_loss = 0.008,  micro/precision = 1.000, micro/recall = 1.000, micro/f1_score = 1.000, macro/precision = 0.143, macro/recall = 0.143, macro/f1_score = 0.143\n",
            "============================================================\n",
            "\n",
            "\n",
            "[Epoch =  2]: val_loss = 0.006,  val_micro/precision = 1.000, val_micro/recall = 1.000, val_micro/f1_score = 1.000, val_macro/precision = 0.143, val_macro/recall = 0.143, val_macro/f1_score = 0.143\n",
            "------------------------------------------------\n",
            "\n",
            "\n",
            "[Epoch =  3]: train_loss = 0.004,  micro/precision = 1.000, micro/recall = 1.000, micro/f1_score = 1.000, macro/precision = 0.143, macro/recall = 0.143, macro/f1_score = 0.143\n",
            "============================================================\n",
            "\n",
            "\n",
            "[Epoch =  3]: val_loss = 0.004,  val_micro/precision = 1.000, val_micro/recall = 1.000, val_micro/f1_score = 1.000, val_macro/precision = 0.143, val_macro/recall = 0.143, val_macro/f1_score = 0.143\n",
            "------------------------------------------------\n",
            "\n",
            "\n",
            "[Epoch =  4]: train_loss = 0.003,  micro/precision = 1.000, micro/recall = 1.000, micro/f1_score = 1.000, macro/precision = 0.143, macro/recall = 0.143, macro/f1_score = 0.143\n",
            "============================================================\n",
            "\n",
            "\n",
            "[Epoch =  4]: val_loss = 0.003,  val_micro/precision = 1.000, val_micro/recall = 1.000, val_micro/f1_score = 1.000, val_macro/precision = 0.143, val_macro/recall = 0.143, val_macro/f1_score = 0.143\n",
            "------------------------------------------------\n",
            "\n",
            "\n",
            "[Epoch =  5]: train_loss = 0.002,  micro/precision = 1.000, micro/recall = 1.000, micro/f1_score = 1.000, macro/precision = 0.143, macro/recall = 0.143, macro/f1_score = 0.143\n",
            "============================================================\n",
            "\n",
            "\n",
            "[Epoch =  5]: val_loss = 0.002,  val_micro/precision = 1.000, val_micro/recall = 1.000, val_micro/f1_score = 1.000, val_macro/precision = 0.143, val_macro/recall = 0.143, val_macro/f1_score = 0.143\n",
            "------------------------------------------------\n",
            "\n",
            "\n",
            "[Epoch =  6]: train_loss = 0.002,  micro/precision = 1.000, micro/recall = 1.000, micro/f1_score = 1.000, macro/precision = 0.143, macro/recall = 0.143, macro/f1_score = 0.143\n",
            "============================================================\n",
            "\n",
            "\n",
            "[Epoch =  6]: val_loss = 0.002,  val_micro/precision = 1.000, val_micro/recall = 1.000, val_micro/f1_score = 1.000, val_macro/precision = 0.143, val_macro/recall = 0.143, val_macro/f1_score = 0.143\n",
            "------------------------------------------------\n",
            "\n",
            "\n",
            "[Epoch =  7]: train_loss = 0.001,  micro/precision = 1.000, micro/recall = 1.000, micro/f1_score = 1.000, macro/precision = 0.143, macro/recall = 0.143, macro/f1_score = 0.143\n",
            "============================================================\n",
            "\n",
            "\n",
            "[Epoch =  7]: val_loss = 0.001,  val_micro/precision = 1.000, val_micro/recall = 1.000, val_micro/f1_score = 1.000, val_macro/precision = 0.143, val_macro/recall = 0.143, val_macro/f1_score = 0.143\n",
            "------------------------------------------------\n",
            "\n",
            "\n",
            "[Epoch =  8]: train_loss = 0.001,  micro/precision = 1.000, micro/recall = 1.000, micro/f1_score = 1.000, macro/precision = 0.143, macro/recall = 0.143, macro/f1_score = 0.143\n",
            "============================================================\n",
            "\n",
            "\n",
            "[Epoch =  8]: val_loss = 0.001,  val_micro/precision = 1.000, val_micro/recall = 1.000, val_micro/f1_score = 1.000, val_macro/precision = 0.143, val_macro/recall = 0.143, val_macro/f1_score = 0.143\n",
            "------------------------------------------------\n",
            "\n",
            "\n",
            "[Epoch =  9]: train_loss = 0.001,  micro/precision = 1.000, micro/recall = 1.000, micro/f1_score = 1.000, macro/precision = 0.143, macro/recall = 0.143, macro/f1_score = 0.143\n",
            "============================================================\n",
            "\n",
            "\n",
            "[Epoch =  9]: val_loss = 0.001,  val_micro/precision = 1.000, val_micro/recall = 1.000, val_micro/f1_score = 1.000, val_macro/precision = 0.143, val_macro/recall = 0.143, val_macro/f1_score = 0.143\n",
            "------------------------------------------------\n",
            "\n",
            "\n",
            "[Epoch =  10]: train_loss = 0.001,  micro/precision = 1.000, micro/recall = 1.000, micro/f1_score = 1.000, macro/precision = 0.143, macro/recall = 0.143, macro/f1_score = 0.143\n",
            "============================================================\n",
            "\n",
            "\n",
            "[Epoch =  10]: val_loss = 0.001,  val_micro/precision = 1.000, val_micro/recall = 1.000, val_micro/f1_score = 1.000, val_macro/precision = 0.143, val_macro/recall = 0.143, val_macro/f1_score = 0.143\n",
            "------------------------------------------------\n",
            "\n",
            "\n",
            "[Epoch =  11]: train_loss = 0.001,  micro/precision = 1.000, micro/recall = 1.000, micro/f1_score = 1.000, macro/precision = 0.143, macro/recall = 0.143, macro/f1_score = 0.143\n",
            "============================================================\n",
            "\n",
            "\n",
            "[Epoch =  11]: val_loss = 0.001,  val_micro/precision = 1.000, val_micro/recall = 1.000, val_micro/f1_score = 1.000, val_macro/precision = 0.143, val_macro/recall = 0.143, val_macro/f1_score = 0.143\n",
            "------------------------------------------------\n",
            "\n",
            "\n",
            "[Epoch =  12]: train_loss = 0.001,  micro/precision = 1.000, micro/recall = 1.000, micro/f1_score = 1.000, macro/precision = 0.143, macro/recall = 0.143, macro/f1_score = 0.143\n",
            "============================================================\n",
            "\n",
            "\n",
            "[Epoch =  12]: val_loss = 0.001,  val_micro/precision = 1.000, val_micro/recall = 1.000, val_micro/f1_score = 1.000, val_macro/precision = 0.143, val_macro/recall = 0.143, val_macro/f1_score = 0.143\n",
            "------------------------------------------------\n",
            "\n",
            "\n",
            "[Epoch =  13]: train_loss = 0.000,  micro/precision = 1.000, micro/recall = 1.000, micro/f1_score = 1.000, macro/precision = 0.143, macro/recall = 0.143, macro/f1_score = 0.143\n",
            "============================================================\n",
            "\n",
            "\n",
            "[Epoch =  13]: val_loss = 0.000,  val_micro/precision = 1.000, val_micro/recall = 1.000, val_micro/f1_score = 1.000, val_macro/precision = 0.143, val_macro/recall = 0.143, val_macro/f1_score = 0.143\n",
            "------------------------------------------------\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7bEtjR03G7b"
      },
      "source": [
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0q-Q7tZJZ22y"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}